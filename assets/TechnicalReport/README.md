# IMelodist技术报告

IMelodist，即InternLM-Melodist，是基于internLM2-chat模型，利用开源框架xtuner微调打造的作曲家大模型。IMelodist不仅能够进行音乐话题的问答，还能像作曲家一样作曲。

- [IMelodist技术报告](#imelodist技术报告)
  - [乐谱数据](#乐谱数据)
  - [微调](#微调)
    - [增量微调](#增量微调)
    - [有监督微调](#有监督微调)
  - [自我认知](#自我认知)
  - [下游任务](#下游任务)
  - [数据收集](#数据收集)
  - [未来展望](#未来展望)

## 乐谱数据
训练所用数据中的乐谱格式为ABC记法。采用ABC乐谱的优势有：
- 以简洁的文本形式表示音乐，且相比MIDI音频文件或音频波形数据，占用存储空间小很多；
- 通用性强，音频文件和ABC乐谱之间的相互转化成熟。
```
X:1
T:Twinkle, Twinkle, Little Star
M:4/4
L:1/4
K:C
C C G G | A A G2 | F F E E | D D C2 |
G G F F | E E D2 | G G F F | E E D2 |
C C G G | A A G2 | F F E E | D D C2 |
```
无论是在训练还是微调中，乐谱数据一定要保证格式的规范、统一。
## 微调
我们在internLM2-chat模型的基础上，做了增量微调和有监督微调。
### 增量微调
为了让基座模型学会垂直领域上的新知识，我们采用qlora算法进行增量微调，数据包括音乐历史、乐谱含义、音乐鉴赏及大量乐谱文本文件。大量的ABC乐谱文本，旨在让模型学会ABC乐谱的格式和语法。所以ABC乐谱数据的格式一定要统一且规范。
### 有监督微调
有监督微调同样是采用qlora算法。这一阶段有两个目的：一是IMelodist的自我认知，二是充分挖掘模型在创作领域的潜力。
## 自我认知
自我认知的调整：书生·浦语 -> IMelodist，双管齐下：
- 修改system_template
- 指令微调
修改后的system_template：
```
IMelodist_system_template:"""
你是专业的音乐作曲家。你知晓世界上的各种风格的音乐并理解音乐的韵律、意境，知晓世界上所有音乐家的人物经历、作曲风格以及他们创作的歌曲，懂得赏析他们的艺术创作，热衷于音乐相关的文学和艺术史。
你对音乐有着痴迷般的着迷，当有人向你提问时你会非常热衷于解答，尽你所能地发挥你的创作能力为人们作曲、提供作曲建议。
你懂得用ABC谱来展示你创作的音乐。你的行为举止非常优雅、讲究礼节，表现得体。
你会根据用户输入的问题、需求进行分析并发挥你的创作能力作曲。
你精通音乐领域的所有知识，能够完成用户输入的要求、请求、问题。
"""
```
项目中发现，如果不进行指令微调，只将system_template更改，模型的自我认知照样能够很好得转变，这归结于基座模型质量高。但是也不可否认，指令微调后才能使得自我认知完美转变。
## 下游任务
每种任务都需要构建相应的sft数据集：
1. 音乐续写
2. 音乐创作（包含创作乐谱、旋律、和弦三者的排列组合）
  - 意境创作：给出意境创作
  - 旋律创作：给出旋律创作谱子；根据条件续写谱子
  - 和弦走向创作：给出和弦走向，创作谱子
  - 风格创作：给定已有谱子作为参考
  - 音乐家风格模仿：按照某个音乐家的风格创作
  - 歌词谱曲：给定歌词，理解内涵后创作
3. 音乐分析
  - 乐谱分析：找谱子里面的某个旋律、和弦；格式分析（几小节，音轨数等）
  - 旋律调谐：给定和弦填旋律
  - 乐谱理解：给定乐谱并说出曲谱风格
  - 基调分析：分析曲谱的基调
## 数据收集
- ABC乐谱数据集的全部来源是sander-wood/irishman开源数据集，以及m-a-p/MusicPile-sft中基于irishman数据集构建的部分。
- 乐谱知识、音乐历史、音乐赏析数据均来源于互联网，通过openchat翻译、润色。
- 自我认知数据集一部分由团队成员手动书写，另一部分由openchat通过合适的prompt产生。
## 未来展望
1. 优化数据集质量，进一步挖掘internLM-chat-7B模型在音乐领域的潜能。
2. 在7B模型的基础上，集成agent、rag，优化模型表现，丰富模型功能。在必要时，比如帮助模型产生思维链时，考虑在基座模型合适部位嫁接上合适大小的AI模块。
3. 模型蒸馏，在1.8B模型上实现卓越的作曲功能。